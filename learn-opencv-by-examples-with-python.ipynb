{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **About OpenCV**\n","* Officially launched in 1999, OpenCV (Open Source Computer Vision) from an Intel initiative.\n","* OpenCV’s core is written in C++. In python we are simply using a wrapper that executes C++ code inside of python.\n","* First major release 1.0 was in 2006, second in 2009, third in 2015 and 4th in 2018. with OpenCV 4.0 Beta.\n","* It is an Open source library containing over 2500 optimized algorithms.\n","* It is EXTREMELY useful for almost all computer vision applications and is supported on Windows, Linux, MacOS, Android, iOS with bindings to Python, Java and Matlab. \n","\n","\n","## Update(19.05.2020)\n","\n","I will always try to improve this kernel. I made some additions to this version. Thanks for reading, I hope it will be useful\n","#### Newly Added Content\n","* 17.Background Subtraction Methods\n","* 18.Funny Mirrors Using OpenCV\n","\n","# **Content**\n","\n","1. [Sharpening](#1.)\n","1. [Thresholding, Binarization & Adaptive Thresholding](#2.)\n","1. [Dilation, Erosion, Opening and Closing](#3.)\n","1. [Edge Detection & Image Gradients](#4.)\n","1. [Perpsective Transform](#5.)\n","1. [Scaling, re-sizing and interpolations](#6.)\n","1. [Image Pyramids](#7.)\n","1. [Cropping](#8.)\n","1. [Blurring](#9.)\n","1. [Contours](#10.)\n","1. [Approximating Contours and Convex Hull](#11.)\n","1. [Identifiy Contours by Shape](#12.)\n","1. [Line Detection - Using Hough Lines](#13.)\n","1. [Counting Circles and Ellipses](#14.)\n","1. [Finding Corners](#15.)\n","1. [Finding Waldo](#16.)\n","1. [Background Subtraction Methods](#17.)\n","1. [Funny Mirrors Using OpenCV](#18.)\n","\n","\n","### Background Subtraction Methods Output\n","![](https://iili.io/JMXhdv.gif)\n","\n","### Funny Mirrors Using OpenCV Output\n","![](https://iili.io/JMw3qF.png)\n","\n","### Some pictures from content\n","![](https://iili.io/JMXPkl.png)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"1.\"></a> \n","# 1.Sharpening\n","By altering our kernels we can implement sharpening, which has the effects of in strengthening or emphasizing edges in an image."]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"ename":"error","evalue":"OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/opencv-samples-images/data/building.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m))\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n","\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"]}],"source":["image = cv2.imread('/kaggle/input/opencv-samples-images/data/building.jpg')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(20, 20))\n","plt.subplot(1, 2, 1)\n","plt.title(\"Original\")\n","plt.imshow(image)\n","\n","\n","# Create our shapening kernel, we don't normalize since the \n","# the values in the matrix sum to 1\n","kernel_sharpening = np.array([[-1,-1,-1], \n","                              [-1,9,-1], \n","                              [-1,-1,-1]])\n","\n","# applying different kernels to the input image\n","sharpened = cv2.filter2D(image, -1, kernel_sharpening)\n","\n","\n","plt.subplot(1, 2, 2)\n","plt.title(\"Image Sharpening\")\n","plt.imshow(sharpened)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"2.\"></a> \n","# 2.Thresholding, Binarization & Adaptive Thresholding"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load our new image\n","image = cv2.imread('/kaggle/input/opencv-samples-images/Origin_of_Species.jpg', 0)\n","\n","plt.figure(figsize=(30, 30))\n","plt.subplot(3, 2, 1)\n","plt.title(\"Original\")\n","plt.imshow(image)\n","\n","# Values below 127 goes to 0 (black, everything above goes to 255 (white)\n","ret,thresh1 = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n","\n","plt.subplot(3, 2, 2)\n","plt.title(\"Threshold Binary\")\n","plt.imshow(thresh1)\n","\n","\n","# It's good practice to blur images as it removes noise\n","image = cv2.GaussianBlur(image, (3, 3), 0)\n","\n","# Using adaptiveThreshold\n","thresh = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 3, 5) \n","\n","plt.subplot(3, 2, 3)\n","plt.title(\"Adaptive Mean Thresholding\")\n","plt.imshow(thresh)\n","\n","\n","_, th2 = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n","\n","plt.subplot(3, 2, 4)\n","plt.title(\"Otsu's Thresholding\")\n","plt.imshow(th2)\n","\n","\n","plt.subplot(3, 2, 5)\n","# Otsu's thresholding after Gaussian filtering\n","blur = cv2.GaussianBlur(image, (5,5), 0)\n","_, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n","plt.title(\"Guassian Otsu's Thresholding\")\n","plt.imshow(th3)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"3.\"></a> \n","# 3.Dilation, Erosion, Opening and Closing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image = cv2.imread('/kaggle/input/opencv-samples-images/data/LinuxLogo.jpg')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(20, 20))\n","plt.subplot(3, 2, 1)\n","plt.title(\"Original\")\n","plt.imshow(image)\n","\n","\n","# Let's define our kernel size\n","kernel = np.ones((5,5), np.uint8)\n","\n","# Now we erode\n","erosion = cv2.erode(image, kernel, iterations = 1)\n","\n","plt.subplot(3, 2, 2)\n","plt.title(\"Erosion\")\n","plt.imshow(erosion)\n","\n","# \n","dilation = cv2.dilate(image, kernel, iterations = 1)\n","plt.subplot(3, 2, 3)\n","plt.title(\"Dilation\")\n","plt.imshow(dilation)\n","\n","\n","# Opening - Good for removing noise\n","opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n","plt.subplot(3, 2, 4)\n","plt.title(\"Opening\")\n","plt.imshow(opening)\n","\n","\n","# Closing - Good for removing noise\n","closing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n","plt.subplot(3, 2, 5)\n","plt.title(\"Closing\")\n","plt.imshow(closing)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"4.\"></a> \n","# 4.Edge Detection & Image Gradients\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image = cv2.imread('/kaggle/input/opencv-samples-images/data/fruits.jpg')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","height, width,_ = image.shape\n","\n","# Extract Sobel Edges\n","sobel_x = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n","sobel_y = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)\n","\n","plt.figure(figsize=(20, 20))\n","\n","plt.subplot(3, 2, 1)\n","plt.title(\"Original\")\n","plt.imshow(image)\n","\n","plt.subplot(3, 2, 2)\n","plt.title(\"Sobel X\")\n","plt.imshow(sobel_x)\n","\n","\n","plt.subplot(3, 2, 3)\n","plt.title(\"Sobel Y\")\n","plt.imshow(sobel_y)\n","\n","sobel_OR = cv2.bitwise_or(sobel_x, sobel_y)\n","\n","plt.subplot(3, 2, 4)\n","plt.title(\"sobel_OR\")\n","plt.imshow(sobel_OR)\n","\n","laplacian = cv2.Laplacian(image, cv2.CV_64F)\n","\n","plt.subplot(3, 2, 5)\n","plt.title(\"Laplacian\")\n","plt.imshow(laplacian)\n","\n","\n","##  Then, we need to provide two values: threshold1 and threshold2. Any gradient value larger than threshold2\n","# is considered to be an edge. Any value below threshold1 is considered not to be an edge. \n","#Values in between threshold1 and threshold2 are either classiﬁed as edges or non-edges based on how their \n","#intensities are “connected”. In this case, any gradient values below 60 are considered non-edges\n","#whereas any values above 120 are considered edges.\n","\n","\n","# Canny Edge Detection uses gradient values as thresholds\n","# The first threshold gradient\n","canny = cv2.Canny(image, 50, 120)\n","\n","plt.subplot(3, 2, 6)\n","plt.title(\"Canny\")\n","plt.imshow(canny)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"5.\"></a> \n","# 5.Perpsective Transform"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image = cv2.imread('/kaggle/input/opencv-samples-images/scan.jpg')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(20, 20))\n","\n","plt.subplot(1, 2, 1)\n","plt.title(\"Original\")\n","plt.imshow(image)\n","\n","# Cordinates of the 4 corners of the original image\n","points_A = np.float32([[320,15], [700,215], [85,610], [530,780]])\n","\n","# Cordinates of the 4 corners of the desired output\n","# We use a ratio of an A4 Paper 1 : 1.41\n","points_B = np.float32([[0,0], [420,0], [0,594], [420,594]])\n"," \n","# Use the two sets of four points to compute \n","# the Perspective Transformation matrix, M    \n","M = cv2.getPerspectiveTransform(points_A, points_B)\n"," \n","warped = cv2.warpPerspective(image, M, (420,594))\n","\n","plt.subplot(1, 2, 2)\n","plt.title(\"warpPerspective\")\n","plt.imshow(warped)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"6.\"></a> \n","# 6.Scaling, re-sizing and interpolations\n","\n","Re-sizing is very easy using the cv2.resize function, it's arguments are: cv2.resize(image, dsize(output image size), x scale, y scale, interpolation)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image = cv2.imread('/kaggle/input/opencv-samples-images/data/fruits.jpg')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(20, 20))\n","\n","plt.subplot(2, 2, 1)\n","plt.title(\"Original\")\n","plt.imshow(image)\n","\n","# Let's make our image 3/4 of it's original size\n","image_scaled = cv2.resize(image, None, fx=0.75, fy=0.75)\n","\n","plt.subplot(2, 2, 2)\n","plt.title(\"Scaling - Linear Interpolation\")\n","plt.imshow(image_scaled)\n","\n","# Let's double the size of our image\n","img_scaled = cv2.resize(image, None, fx=2, fy=2, interpolation = cv2.INTER_CUBIC)\n","\n","plt.subplot(2, 2, 3)\n","plt.title(\"Scaling - Cubic Interpolation\")\n","plt.imshow(img_scaled)\n","\n","# Let's skew the re-sizing by setting exact dimensions\n","img_scaled = cv2.resize(image, (900, 400), interpolation = cv2.INTER_AREA)\n","\n","plt.subplot(2, 2, 4)\n","plt.title(\"Scaling - Skewed Size\")\n","plt.imshow(img_scaled)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"7.\"></a> \n","# 7.Image Pyramids\n","Useful when scaling images in object detection."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image = cv2.imread('/kaggle/input/opencv-samples-images/data/butterfly.jpg')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(20, 20))\n","\n","plt.subplot(2, 2, 1)\n","plt.title(\"Original\")\n","plt.imshow(image)\n","\n","smaller = cv2.pyrDown(image)\n","larger = cv2.pyrUp(smaller)\n","\n","plt.subplot(2, 2, 2)\n","plt.title(\"Smaller\")\n","plt.imshow(smaller)\n","\n","plt.subplot(2, 2, 3)\n","plt.title(\"Larger\")\n","plt.imshow(larger)\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"8.\"></a> \n","# 8.Cropping"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image = cv2.imread('/kaggle/input/opencv-samples-images/data/messi5.jpg')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(20, 20))\n","\n","plt.subplot(2, 2, 1)\n","plt.title(\"Original\")\n","plt.imshow(image)\n","\n","height, width = image.shape[:2]\n","\n","# Let's get the starting pixel coordiantes (top  left of cropping rectangle)\n","start_row, start_col = int(height * .25), int(width * .25)\n","\n","# Let's get the ending pixel coordinates (bottom right)\n","end_row, end_col = int(height * .75), int(width * .75)\n","\n","# Simply use indexing to crop out the rectangle we desire\n","cropped = image[start_row:end_row , start_col:end_col]\n","\n","\n","plt.subplot(2, 2, 2)\n","plt.title(\"Cropped\")\n","plt.imshow(cropped)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"9.\"></a> \n","# 9.Blurring\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image = cv2.imread('/kaggle/input/opencv-samples-images/data/home.jpg')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(20, 20))\n","\n","plt.subplot(2, 2, 1)\n","plt.title(\"Original\")\n","plt.imshow(image)\n","\n","# Creating our 3 x 3 kernel\n","kernel_3x3 = np.ones((3, 3), np.float32) / 9\n","\n","# We use the cv2.fitler2D to conovlve the kernal with an image \n","blurred = cv2.filter2D(image, -1, kernel_3x3)\n","\n","plt.subplot(2, 2, 2)\n","plt.title(\"3x3 Kernel Blurring\")\n","plt.imshow(blurred)\n","\n","# Creating our 7 x 7 kernel\n","kernel_7x7 = np.ones((7, 7), np.float32) / 49\n","\n","blurred2 = cv2.filter2D(image, -1, kernel_7x7)\n","\n","plt.subplot(2, 2, 3)\n","plt.title(\"7x7 Kernel Blurring\")\n","plt.imshow(blurred2)\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"10.\"></a> \n","# 10.Contours\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Let's load a simple image with 3 black squares\n","image = cv2.imread('/kaggle/input/opencv-samples-images/data/pic3.png')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(20, 20))\n","\n","plt.subplot(2, 2, 1)\n","plt.title(\"Original\")\n","plt.imshow(image)\n","\n","\n","# Grayscale\n","gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n","\n","# Find Canny edges\n","edged = cv2.Canny(gray, 30, 200)\n","\n","plt.subplot(2, 2, 2)\n","plt.title(\"Canny Edges\")\n","plt.imshow(edged)\n","\n","\n","# Finding Contours\n","# Use a copy of your image e.g. edged.copy(), since findContours alters the image\n","contours, hierarchy = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n","\n","plt.subplot(2, 2, 3)\n","plt.title(\"Canny Edges After Contouring\")\n","plt.imshow(edged)\n","\n","print(\"Number of Contours found = \" + str(len(contours)))\n","\n","# Draw all contours\n","# Use '-1' as the 3rd parameter to draw all\n","cv2.drawContours(image, contours, -1, (0,255,0), 3)\n","\n","plt.subplot(2, 2, 4)\n","plt.title(\"Contours\")\n","plt.imshow(image)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"11.\"></a> \n","# 11.Approximating Contours and Convex Hull\n","\n","cv2.approxPolyDP(contour, Approximation Accuracy, Closed)\n","\n","* contour – is the individual contour we wish to approximate\n","* Approximation Accuracy – Important parameter is determining the accuracy of the approximation. Small values give precise- approximations, large values give more generic approximation. A good rule of thumb is less than 5% of the contour perimeter\n","* Closed – a Boolean value that states whether the approximate contour should be open or closed\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load image and keep a copy\n","image = cv2.imread('/kaggle/input/opencv-samples-images/house.jpg')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(20, 20))\n","\n","plt.subplot(2, 2, 1)\n","plt.title(\"Original\")\n","plt.imshow(image)\n","\n","orig_image = image.copy()\n","\n","\n","# Grayscale and binarize\n","gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","ret, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n","\n","# Find contours \n","contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n","\n","# Iterate through each contour and compute the bounding rectangle\n","for c in contours:\n","    x,y,w,h = cv2.boundingRect(c)\n","    cv2.rectangle(orig_image,(x,y),(x+w,y+h),(0,0,255),2)    \n","    plt.subplot(2, 2, 2)\n","    plt.title(\"Bounding Rectangle\")\n","    plt.imshow(orig_image)\n","\n","cv2.waitKey(0) \n","    \n","# Iterate through each contour and compute the approx contour\n","for c in contours:\n","    # Calculate accuracy as a percent of the contour perimeter\n","    accuracy = 0.03 * cv2.arcLength(c, True)\n","    approx = cv2.approxPolyDP(c, accuracy, True)\n","    cv2.drawContours(image, [approx], 0, (0, 255, 0), 2)\n","    \n","    plt.subplot(2, 2, 3)\n","    plt.title(\"Approx Poly DP\")\n","    plt.imshow(image)\n","\n","plt.show()\n","    \n","# Convex Hull\n","\n","\n","image = cv2.imread('/kaggle/input/opencv-samples-images/hand.jpg')\n","gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","plt.figure(figsize=(20, 20))\n","\n","plt.subplot(1, 2, 1)\n","plt.title(\"Original Image\")\n","plt.imshow(image)\n","\n","# Threshold the image\n","ret, thresh = cv2.threshold(gray, 176, 255, 0)\n","\n","# Find contours \n","contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n","    \n","# Sort Contors by area and then remove the largest frame contour\n","n = len(contours) - 1\n","contours = sorted(contours, key=cv2.contourArea, reverse=False)[:n]\n","\n","# Iterate through contours and draw the convex hull\n","for c in contours:\n","    hull = cv2.convexHull(c)\n","    cv2.drawContours(image, [hull], 0, (0, 255, 0), 2)\n","\n","    plt.subplot(1, 2, 2)\n","    plt.title(\"Convex Hull\")\n","    plt.imshow(image)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"12.\"></a> \n","# 12.Identifiy Contours by Shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image = cv2.imread('/kaggle/input/opencv-samples-images/someshapes.jpg')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","plt.figure(figsize=(20, 20))\n","\n","plt.subplot(2, 2, 1)\n","plt.title(\"Original\")\n","plt.imshow(image)\n","\n","ret, thresh = cv2.threshold(gray, 127, 255, 1)\n","\n","# Extract Contours\n","contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n","\n","for cnt in contours:\n","    \n","    # Get approximate polygons\n","    approx = cv2.approxPolyDP(cnt, 0.01*cv2.arcLength(cnt,True),True)\n","    \n","    if len(approx) == 3:\n","        shape_name = \"Triangle\"\n","        cv2.drawContours(image,[cnt],0,(0,255,0),-1)\n","        \n","        # Find contour center to place text at the center\n","        M = cv2.moments(cnt)\n","        cx = int(M['m10'] / M['m00'])\n","        cy = int(M['m01'] / M['m00'])\n","        cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n","    \n","    elif len(approx) == 4:\n","        x,y,w,h = cv2.boundingRect(cnt)\n","        M = cv2.moments(cnt)\n","        cx = int(M['m10'] / M['m00'])\n","        cy = int(M['m01'] / M['m00'])\n","        \n","        # Check to see if 4-side polygon is square or rectangle\n","        # cv2.boundingRect returns the top left and then width and \n","        if abs(w-h) <= 3:\n","            shape_name = \"Square\"\n","            \n","            # Find contour center to place text at the center\n","            cv2.drawContours(image, [cnt], 0, (0, 125 ,255), -1)\n","            cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n","        else:\n","            shape_name = \"Rectangle\"\n","            \n","            # Find contour center to place text at the center\n","            cv2.drawContours(image, [cnt], 0, (0, 0, 255), -1)\n","            M = cv2.moments(cnt)\n","            cx = int(M['m10'] / M['m00'])\n","            cy = int(M['m01'] / M['m00'])\n","            cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n","            \n","    elif len(approx) == 10:\n","        shape_name = \"Star\"\n","        cv2.drawContours(image, [cnt], 0, (255, 255, 0), -1)\n","        M = cv2.moments(cnt)\n","        cx = int(M['m10'] / M['m00'])\n","        cy = int(M['m01'] / M['m00'])\n","        cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n","        \n","        \n","        \n","    elif len(approx) >= 15:\n","        shape_name = \"Circle\"\n","        cv2.drawContours(image, [cnt], 0, (0, 255, 255), -1)\n","        M = cv2.moments(cnt)\n","        cx = int(M['m10'] / M['m00'])\n","        cy = int(M['m01'] / M['m00'])\n","        cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n","\n","plt.subplot(2, 2, 2)\n","plt.title(\"Identifying Shapes\")\n","plt.imshow(image)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"13.\"></a> \n","# 13.Line Detection - Using Hough Lines\n","\n","cv2.HoughLines(binarized/thresholded image, 𝜌 accuracy, 𝜃 accuracy, threshold)\n","\n","* Threshold here is the minimum vote for it to be considered a line\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image = cv2.imread('/kaggle/input/opencv-samples-images/data/sudoku.png')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(20, 20))\n","\n","# Grayscale and Canny Edges extracted\n","gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","edges = cv2.Canny(gray, 100, 170, apertureSize = 3)\n","\n","plt.subplot(2, 2, 1)\n","plt.title(\"edges\")\n","plt.imshow(edges)\n","\n","# Run HoughLines using a rho accuracy of 1 pixel\n","# theta accuracy of np.pi / 180 which is 1 degree\n","# Our line threshold is set to 240 (number of points on line)\n","lines = cv2.HoughLines(edges, 1, np.pi/180, 200)\n","\n","# We iterate through each line and convert it to the format\n","# required by cv.lines (i.e. requiring end points)\n","for line in lines:\n","    rho, theta = line[0]\n","    a = np.cos(theta)\n","    b = np.sin(theta)\n","    x0 = a * rho\n","    y0 = b * rho\n","    x1 = int(x0 + 1000 * (-b))\n","    y1 = int(y0 + 1000 * (a))\n","    x2 = int(x0 - 1000 * (-b))\n","    y2 = int(y0 - 1000 * (a))\n","    cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n","\n","\n","plt.subplot(2, 2, 2)\n","plt.title(\"Hough Lines\")\n","plt.imshow(image)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"14.\"></a> \n","# 14.Counting Circles and Ellipses"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image = cv2.imread('/kaggle/input/opencv-samples-images/blobs.jpg')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(20, 20))\n","\n","\n","# Intialize the detector using the default parameters\n","detector = cv2.SimpleBlobDetector_create()\n"," \n","# Detect blobs\n","keypoints = detector.detect(image)\n"," \n","# Draw blobs on our image as red circles\n","blank = np.zeros((1,1)) \n","blobs = cv2.drawKeypoints(image, keypoints, blank, (0,0,255),\n","                                      cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n","\n","number_of_blobs = len(keypoints)\n","text = \"Total Number of Blobs: \" + str(len(keypoints))\n","cv2.putText(blobs, text, (20, 550), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 0, 255), 2)\n","\n","# Display image with blob keypoints\n","plt.subplot(2, 2, 1)\n","plt.title(\"Blobs using default parameters\")\n","plt.imshow(blobs)\n","\n","\n","# Set our filtering parameters\n","# Initialize parameter settiing using cv2.SimpleBlobDetector\n","params = cv2.SimpleBlobDetector_Params()\n","\n","# Set Area filtering parameters\n","params.filterByArea = True\n","params.minArea = 100\n","\n","# Set Circularity filtering parameters\n","params.filterByCircularity = True \n","params.minCircularity = 0.9\n","\n","# Set Convexity filtering parameters\n","params.filterByConvexity = False\n","params.minConvexity = 0.2\n","    \n","# Set inertia filtering parameters\n","params.filterByInertia = True\n","params.minInertiaRatio = 0.01\n","\n","# Create a detector with the parameters\n","detector = cv2.SimpleBlobDetector_create(params)\n","    \n","# Detect blobs\n","keypoints = detector.detect(image)\n","\n","# Draw blobs on our image as red circles\n","blank = np.zeros((1,1)) \n","blobs = cv2.drawKeypoints(image, keypoints, blank, (0,255,0),\n","                                      cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n","\n","number_of_blobs = len(keypoints)\n","text = \"Number of Circular Blobs: \" + str(len(keypoints))\n","cv2.putText(blobs, text, (20, 550), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 100, 255), 2)\n","\n","# Show blobs\n","plt.subplot(2, 2, 2)\n","plt.title(\"Filtering Circular Blobs Only\")\n","plt.imshow(blobs)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"15.\"></a> \n","# 15.Finding Corners"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load image then grayscale\n","image = cv2.imread('/kaggle/input/opencv-samples-images/data/chessboard.png')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(10, 10))\n","\n","gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","# The cornerHarris function requires the array datatype to be float32\n","gray = np.float32(gray)\n","\n","harris_corners = cv2.cornerHarris(gray, 3, 3, 0.05)\n","\n","#We use dilation of the corner points to enlarge them\\\n","kernel = np.ones((7,7),np.uint8)\n","harris_corners = cv2.dilate(harris_corners, kernel, iterations = 10)\n","\n","# Threshold for an optimal value, it may vary depending on the image.\n","image[harris_corners > 0.025 * harris_corners.max() ] = [255, 127, 127]\n","\n","plt.subplot(1, 1, 1)\n","plt.title(\"Harris Corners\")\n","plt.imshow(image)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"16.\"></a> \n","# 16.Finding Waldo"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load input image and convert to grayscale\n","image = cv2.imread('/kaggle/input/opencv-samples-images/WaldoBeach.jpg')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(30, 30))\n","\n","plt.subplot(2, 2, 1)\n","plt.title(\"Where is Waldo?\")\n","plt.imshow(image)\n","\n","gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","# Load Template image\n","template = cv2.imread('/kaggle/input/opencv-samples-images/waldo.jpg',0)\n","\n","result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)\n","min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n","\n","#Create Bounding Box\n","top_left = max_loc\n","bottom_right = (top_left[0] + 50, top_left[1] + 50)\n","cv2.rectangle(image, top_left, bottom_right, (0,0,255), 5)\n","\n","plt.subplot(2, 2, 2)\n","plt.title(\"Waldo\")\n","plt.imshow(image)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"17.\"></a> \n","# 17.Background Subtraction Methods"]},{"cell_type":"markdown","metadata":{},"source":["source: https://docs.opencv.org/3.4/d1/dc5/tutorial_background_subtraction.html\n","\n","## How to Use Background Subtraction Methods\n","\n","Background subtraction (BS) is a common and widely used technique for generating a foreground mask (namely, a binary image containing the pixels belonging to moving objects in the scene) by using static cameras.\n","\n","As the name suggests, BS calculates the foreground mask performing a subtraction between the current frame and a background model, containing the static part of the scene or, more in general, everything that can be considered as background given the characteristics of the observed scene.\n","\n","![](https://docs.opencv.org/3.4/Background_Subtraction_Tutorial_Scheme.png)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import cv2 \n","import matplotlib.pyplot as plt\n","\n","algo = 'MOG2'\n","\n","if algo == 'MOG2':\n","    backSub = cv2.createBackgroundSubtractorMOG2()\n","else:\n","    backSub = cv2.createBackgroundSubtractorKNN()\n","\n","plt.figure(figsize=(20, 20))\n","\n","frame = cv2.imread('/kaggle/input/opencv-samples-images/Background_Subtraction_Tutorial_frame.png')\n","fgMask = backSub.apply(frame)\n","\n","plt.subplot(2, 2, 1)\n","plt.title(\"Frame\")\n","plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","\n","plt.subplot(2, 2, 2)\n","plt.title(\"FG Mask\")\n","plt.imshow(cv2.cvtColor(fgMask, cv2.COLOR_BGR2RGB))\n","                       \n","frame = cv2.imread('/kaggle/input/opencv-samples-images/Background_Subtraction_Tutorial_frame_1.png')\n","fgMask = backSub.apply(frame)\n","\n","plt.subplot(2, 2, 3)\n","plt.title(\"Frame\")\n","plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","\n","plt.subplot(2, 2, 4)\n","plt.title(\"FG Mask\")\n","plt.imshow(cv2.cvtColor(fgMask, cv2.COLOR_BGR2RGB))"]},{"cell_type":"markdown","metadata":{},"source":["## If you want to run it on video and locally, you must set it to (While) True. (Do not try on Kaggle you will get the error)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","algo = 'MOG2'\n","inputt = '/kaggle/input/opencv-samples-images/video_input/Background_Subtraction_Tutorial_frame.mp4'\n","\n","capture = cv2.VideoCapture(cv2.samples.findFileOrKeep(inputt))\n","frame_width = int(capture.get(3))\n","frame_height = int(capture.get(4))\n","\n","out = cv2.VideoWriter('Background_Subtraction_Tutorial_frame_output.mp4',cv2.VideoWriter_fourcc('M','J','P','G'),30, (frame_width,frame_height))\n","\n","if algo == 'MOG2':\n","    backSub = cv2.createBackgroundSubtractorMOG2()\n","else:\n","    backSub = cv2.createBackgroundSubtractorKNN()\n","\n","# If you want to run it on video and locally, you must set it to (While) True. (Do not try on Kaggle you will get the error)\n","while False:\n","    \n","    ret, frame = capture.read()\n","    \n","    if frame is None:\n","        break\n","    \n","    fgMask = backSub.apply(frame)\n","    \n","    cv2.rectangle(frame, (10, 2), (100,20), (255,255,255), -1)\n","\n","    cv2.imshow('Frame', frame)\n","    cv2.imshow('FG Mask', fgMask)\n","    \n","    out.write(cv2.cvtColor(fgMask, cv2.COLOR_BGR2RGB))\n","    \n","    keyboard = cv2.waitKey(1) & 0xFF;\n","        \n","    if (keyboard == 27 or keyboard == ord('q')):\n","        cv2.destroyAllWindows()\n","        break;\n","        \n","capture.release()\n","out.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{},"source":["## The result you will get on video and locally\n","\n","![](https://iili.io/JMXhdv.gif)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"18.\"></a> \n","# 18.Funny Mirrors Using OpenCV\n","Source: https://www.learnopencv.com/funny-mirrors-using-opencv/\n","\n","Funny mirrors are not plane mirrors but a combination of convex/concave reflective surfaces that produce distortion effects that look funny as we move in front of these mirrors.\n","\n","### How does it work ?\n","The entire project can be divided into three major steps :\n","\n","* Create a virtual camera.\n","* Define a 3D surface (the mirror surface) and project it into the virtual camera using a suitable value of projection matrix.\n","* Use the image coordinates of the projected points of the 3D surface to apply mesh based warping to get the desired effect of a funny mirror.\n","\n","![](https://www.learnopencv.com/wp-content/uploads/2020/04/steps-for-funny-mirrors.jpg)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install vcam"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","import math\n","from vcam import vcam,meshGen\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(20, 20))\n","\n","# Reading the input image. Pass the path of image you would like to use as input image.\n","img = cv2.imread(\"/kaggle/input/opencv-samples-images/minions.jpg\")\n","H,W = img.shape[:2]\n","\n","# Creating the virtual camera object\n","c1 = vcam(H=H,W=W)\n","\n","# Creating the surface object\n","plane = meshGen(H,W)\n","\n","# We generate a mirror where for each 3D point, its Z coordinate is defined as Z = 20*exp^((x/w)^2 / 2*0.1*sqrt(2*pi))\n","\n","plane.Z += 20*np.exp(-0.5*((plane.X*1.0/plane.W)/0.1)**2)/(0.1*np.sqrt(2*np.pi))\n","pts3d = plane.getPlane()\n","\n","pts2d = c1.project(pts3d)\n","map_x,map_y = c1.getMaps(pts2d)\n","\n","output = cv2.remap(img,map_x,map_y,interpolation=cv2.INTER_LINEAR)\n","\n","plt.subplot(1, 2,1)\n","plt.title(\"Funny Mirror\")\n","plt.imshow(cv2.cvtColor(np.hstack((img,output)), cv2.COLOR_BGR2RGB))"]},{"cell_type":"markdown","metadata":{},"source":["So now as we know that by defining Z as a function of X and Y we can create different types of distortion effects. Let us create some more effects using the above code. We simply need to change the line where we define Z as a function of X and Y. This will further help you to create your own effects."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20, 20))\n","\n","# Reading the input image. Pass the path of image you would like to use as input image.\n","img = cv2.imread(\"/kaggle/input/opencv-samples-images/minions.jpg\")\n","H,W = img.shape[:2]\n","\n","# Creating the virtual camera object\n","c1 = vcam(H=H,W=W)\n","\n","# Creating the surface object\n","plane = meshGen(H,W)\n","\n","# We generate a mirror where for each 3D point, its Z coordinate is defined as Z = 20*exp^((y/h)^2 / 2*0.1*sqrt(2*pi))\n","plane.Z += 20*np.exp(-0.5*((plane.Y*1.0/plane.H)/0.1)**2)/(0.1*np.sqrt(2*np.pi))\n","\n","pts3d = plane.getPlane()\n","\n","pts2d = c1.project(pts3d)\n","map_x,map_y = c1.getMaps(pts2d)\n","\n","output = cv2.remap(img,map_x,map_y,interpolation=cv2.INTER_LINEAR)\n","\n","plt.subplot(1, 2,1)\n","plt.title(\"Funny Mirror\")\n","plt.imshow(cv2.cvtColor(np.hstack((img,output)), cv2.COLOR_BGR2RGB))"]},{"cell_type":"markdown","metadata":{},"source":["Let’s create something using sine function !"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20, 20))\n","\n","# Reading the input image. Pass the path of image you would like to use as input image.\n","img = cv2.imread(\"/kaggle/input/opencv-samples-images/minions.jpg\")\n","H,W = img.shape[:2]\n","\n","# Creating the virtual camera object\n","c1 = vcam(H=H,W=W)\n","\n","# Creating the surface object\n","plane = meshGen(H,W)\n","\n","# We generate a mirror where for each 3D point, its Z coordinate is defined as Z = 20*[ sin(2*pi*(x/w-1/4))) + sin(2*pi*(y/h-1/4))) ]\n","\n","plane.Z += 20*np.sin(2*np.pi*((plane.X-plane.W/4.0)/plane.W)) + 20*np.sin(2*np.pi*((plane.Y-plane.H/4.0)/plane.H))\n","\n","pts3d = plane.getPlane()\n","\n","pts2d = c1.project(pts3d)\n","map_x,map_y = c1.getMaps(pts2d)\n","\n","output = cv2.remap(img,map_x,map_y,interpolation=cv2.INTER_LINEAR)\n","\n","plt.subplot(1, 2,1)\n","plt.title(\"Funny Mirror\")\n","plt.imshow(cv2.cvtColor(np.hstack((img,output)), cv2.COLOR_BGR2RGB))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":501611,"sourceId":1169606,"sourceType":"datasetVersion"}],"dockerImageVersionId":29852,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":4}
